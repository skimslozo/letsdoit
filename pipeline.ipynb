{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import warnings\n",
    "\n",
    "cwd = '/teamspace/studios/this_studio/letsdoit'\n",
    "if (cwd not in sys.path):\n",
    "    sys.path.append(cwd)\n",
    "sys.path.append(os.path.dirname(cwd))\n",
    "\n",
    "from pipeline.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config = '/teamspace/studios/this_studio/letsdoit/config/config_dev.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_config, 'r') as file:\n",
    "    cfg = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ClipRetriver...\n",
      "Initializing the MasksFinder...\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "Initializing MasksMerger...\n",
      "Initializing ClipModel...\n",
      "Initializing ClipProcessor...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline(**cfg, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_list = pipe.instruction_list\n",
    "instruction_block = instruction_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading rgb frames from visit 420683 and video 42445132: 100%|██████████| 159/159 [00:00<00:00, 153344.29it/s]\n",
      "Loading rgb frames from visit 420683 and video 42445135: 100%|██████████| 174/174 [00:00<00:00, 170995.52it/s]\n",
      "Loading rgb frames from visit 420683 and video 42445137: 100%|██████████| 440/440 [00:00<00:00, 204079.81it/s]\n",
      "Loading depth frames from visit 420683 and video 42445132: 100%|██████████| 159/159 [00:00<00:00, 187382.51it/s]\n",
      "Loading depth frames from visit 420683 and video 42445135: 100%|██████████| 174/174 [00:00<00:00, 169841.49it/s]\n",
      "Loading depth frames from visit 420683 and video 42445137: 100%|██████████| 440/440 [00:00<00:00, 203876.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting objects for 'knob'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting the masks: 100%|██████████| 10/10 [00:12<00:00,  1.22s/it]\n",
      "Merging the object instances: 100%|██████████| 40/40 [00:29<00:00,  1.37it/s]\n",
      "Denoising point clouds: 100%|██████████| 15/15 [00:07<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting objects for 'drawer'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting the masks: 100%|██████████| 10/10 [00:11<00:00,  1.17s/it]\n",
      "Merging the object instances: 100%|██████████| 27/27 [00:28<00:00,  1.06s/it]\n",
      "Denoising point clouds: 100%|██████████| 13/13 [00:02<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting objects for 'nightstand'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting the masks:  30%|███       | 3/10 [00:03<00:09,  1.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/teamspace/studios/this_studio/letsdoit/pipeline.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bssh.lightning.ai/teamspace/studios/this_studio/letsdoit/pipeline.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m action_object \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39;49m_run_instruction_block(instruction_block)\n",
      "File \u001b[0;32m~/letsdoit/pipeline/pipeline.py:111\u001b[0m, in \u001b[0;36mPipeline._run_instruction_block\u001b[0;34m(self, instruction_block)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mfor\u001b[39;00m object_label \u001b[39min\u001b[39;00m object_labels:\n\u001b[1;32m    110\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExtracting objects for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mobject_label\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 111\u001b[0m     objects\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_objects_3d(object_label, dict_data))\n\u001b[1;32m    113\u001b[0m action_objects \u001b[39m=\u001b[39m []\n\u001b[1;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m instruction \u001b[39min\u001b[39;00m instructions:\n",
      "File \u001b[0;32m~/letsdoit/pipeline/pipeline.py:185\u001b[0m, in \u001b[0;36mPipeline._get_objects_3d\u001b[0;34m(self, object_label, dict_data)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_objects_3d\u001b[39m(\u001b[39mself\u001b[39m, object_label: \u001b[39mstr\u001b[39m, dict_data: \u001b[39mdict\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Object3D]:\n\u001b[0;32m--> 185\u001b[0m     object_instances \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_object_instances(object_label, dict_data)\n\u001b[1;32m    186\u001b[0m     objects_3d \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasks_merger(object_instances, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpcd, object_proximity_thresh\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobject_proximity_thresh)\n\u001b[1;32m    187\u001b[0m     denoise_objects_3d(objects_3d)\n",
      "File \u001b[0;32m~/letsdoit/pipeline/pipeline.py:202\u001b[0m, in \u001b[0;36mPipeline._get_object_instances\u001b[0;34m(self, object_label, dict_data)\u001b[0m\n\u001b[1;32m    199\u001b[0m best_depth_paths \u001b[39m=\u001b[39m select_ids(dict_data[\u001b[39m'\u001b[39m\u001b[39mdepth_paths\u001b[39m\u001b[39m'\u001b[39m], best_indices)\n\u001b[1;32m    201\u001b[0m \u001b[39m# Masks we get here as outputs are for the upright-rotated images\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m image_ids, masks, bboxes, confidences, labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmasks_finder\u001b[39m.\u001b[39;49mget_masks_from_imgs(best_images_rotated, object_label)\n\u001b[1;32m    204\u001b[0m \u001b[39m# Rotate masks and bboxes back to the rotation of the original image\u001b[39;00m\n\u001b[1;32m    205\u001b[0m mask_image_sizes \u001b[39m=\u001b[39m [best_images_rotated[idx]\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m image_ids]\n",
      "File \u001b[0;32m~/letsdoit/pipeline/masks_finder.py:74\u001b[0m, in \u001b[0;36mMasksFinder.get_masks_from_imgs\u001b[0;34m(self, images, objects)\u001b[0m\n\u001b[1;32m     72\u001b[0m labels \u001b[39m=\u001b[39m []\n\u001b[1;32m     73\u001b[0m \u001b[39mfor\u001b[39;00m image_idx, image \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(images, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mExtracting the masks\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m---> 74\u001b[0m     masks_, bboxes_, confidences_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_masks_from_img(image, objects)\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(masks_) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     76\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/letsdoit/pipeline/masks_finder.py:108\u001b[0m, in \u001b[0;36mMasksFinder.get_masks_from_img\u001b[0;34m(self, image, objects)\u001b[0m\n\u001b[1;32m    105\u001b[0m     boxes_filt[i][\u001b[39m2\u001b[39m:] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m boxes_filt[i][:\u001b[39m2\u001b[39m]\n\u001b[1;32m    106\u001b[0m     boxes_filt[i] \u001b[39m=\u001b[39m constrain_box(boxes_filt[i], (W, H))\n\u001b[0;32m--> 108\u001b[0m masks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_masks(image_cv2, boxes_filt)\n\u001b[1;32m    109\u001b[0m masks \u001b[39m=\u001b[39m [mask\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m mask \u001b[39min\u001b[39;00m masks]\n\u001b[1;32m    110\u001b[0m bboxes \u001b[39m=\u001b[39m [bbox\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39mfor\u001b[39;00m bbox \u001b[39min\u001b[39;00m boxes_filt]\n",
      "File \u001b[0;32m~/letsdoit/pipeline/masks_finder.py:121\u001b[0m, in \u001b[0;36mMasksFinder._predict_masks\u001b[0;34m(self, image_cv2, boxes_filt)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict_masks\u001b[39m(\u001b[39mself\u001b[39m, image_cv2, boxes_filt):\n\u001b[0;32m--> 121\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask_predictor\u001b[39m.\u001b[39;49mset_image(image_cv2)\n\u001b[1;32m    122\u001b[0m     transformed_boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_predictor\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mapply_boxes_torch(boxes_filt, \n\u001b[1;32m    123\u001b[0m                                                                         image_cv2\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    125\u001b[0m     masks, _, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_predictor\u001b[39m.\u001b[39mpredict_torch(\n\u001b[1;32m    126\u001b[0m         point_coords \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m         point_labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m         boxes \u001b[39m=\u001b[39m transformed_boxes\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m    129\u001b[0m         multimask_output \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    130\u001b[0m     )\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/segment_anything/segment_anything/predictor.py:62\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     59\u001b[0m input_image_torch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(input_image, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     60\u001b[0m input_image_torch \u001b[39m=\u001b[39m input_image_torch\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()[\u001b[39mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 62\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_torch_image(input_image_torch, image\u001b[39m.\u001b[39;49mshape[:\u001b[39m2\u001b[39;49m])\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/segment_anything/segment_anything/predictor.py:91\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(transformed_image\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:])\n\u001b[1;32m     90\u001b[0m input_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterm_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mimage_encoder(input_image)\n\u001b[1;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_image_set \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/segment_anything/segment_anything/modeling/image_encoder.py:113\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m interm_embeddings\u001b[39m=\u001b[39m[]\n\u001b[1;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 113\u001b[0m     x \u001b[39m=\u001b[39m blk(x)\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m blk\u001b[39m.\u001b[39mwindow_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    115\u001b[0m         interm_embeddings\u001b[39m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/segment_anything/segment_anything/modeling/image_encoder.py:177\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    174\u001b[0m     H, W \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]\n\u001b[1;32m    175\u001b[0m     x, pad_hw \u001b[39m=\u001b[39m window_partition(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size)\n\u001b[0;32m--> 177\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x)\n\u001b[1;32m    178\u001b[0m \u001b[39m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/segment_anything/segment_anything/modeling/image_encoder.py:237\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    234\u001b[0m attn \u001b[39m=\u001b[39m (q \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale) \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 237\u001b[0m     attn \u001b[39m=\u001b[39m add_decomposed_rel_pos(attn, q, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrel_pos_h, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrel_pos_w, (H, W), (H, W))\n\u001b[1;32m    239\u001b[0m attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39msoftmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    240\u001b[0m x \u001b[39m=\u001b[39m (attn \u001b[39m@\u001b[39m v)\u001b[39m.\u001b[39mview(B, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, H, W, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\u001b[39m.\u001b[39mreshape(B, H, W, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Grounded-Segment-Anything/segment_anything/segment_anything/modeling/image_encoder.py:352\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    350\u001b[0m q_h, q_w \u001b[39m=\u001b[39m q_size\n\u001b[1;32m    351\u001b[0m k_h, k_w \u001b[39m=\u001b[39m k_size\n\u001b[0;32m--> 352\u001b[0m Rh \u001b[39m=\u001b[39m get_rel_pos(q_h, k_h, rel_pos_h)\n\u001b[1;32m    353\u001b[0m Rw \u001b[39m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    355\u001b[0m B, _, dim \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "action_object = pipe._run_instruction_block(instruction_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading rgb frames from visit 420683 and video 42445137: 100%|██████████| 88/88 [00:04<00:00, 20.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['42445137']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading depth frames from visit 420683 and video 42445137: 100%|██████████| 88/88 [00:03<00:00, 27.70it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m action_objects \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/letsdoit/pipeline/pipeline.py:83\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, debug)\u001b[0m\n\u001b[1;32m     81\u001b[0m         action_objects\u001b[38;5;241m.\u001b[39mappend(ao)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_instruction_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction_block\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action_objects\n",
      "File \u001b[0;32m~/letsdoit/pipeline/pipeline.py:94\u001b[0m, in \u001b[0;36mPipeline._run_instruction_block\u001b[0;34m(self, instruction_block, debug)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpcd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mload_pcd(visit_id)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Get both original and upright-rotated images and depths as outputs\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m dict_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisit_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever\u001b[38;5;241m.\u001b[39mgenerate_image_features(dict_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages_rotated\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     97\u001b[0m object_labels \u001b[38;5;241m=\u001b[39m instruction_block[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_objects\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/letsdoit/pipeline/pipeline.py:160\u001b[0m, in \u001b[0;36mPipeline._load_data\u001b[0;34m(self, visit_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, visit_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    157\u001b[0m     images, images_rotated, image_paths, intrinsics, poses, orientations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader\u001b[38;5;241m.\u001b[39mget_images(visit_id, \n\u001b[1;32m    158\u001b[0m                                                                                                   asset_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_asset_type, \n\u001b[1;32m    159\u001b[0m                                                                                                   sample_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_sample_freq)\n\u001b[0;32m--> 160\u001b[0m     depths, depths_rotated, depth_paths, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_depths\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisit_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                                                                          \u001b[49m\u001b[43masset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_asset_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m                                                                          \u001b[49m\u001b[43msample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_sample_freq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     dict_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m: images,\n\u001b[1;32m    165\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages_rotated\u001b[39m\u001b[38;5;124m'\u001b[39m: images_rotated,\n\u001b[1;32m    166\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_paths\u001b[39m\u001b[38;5;124m'\u001b[39m: image_paths,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepths_rotated\u001b[39m\u001b[38;5;124m'\u001b[39m: depths_rotated,\n\u001b[1;32m    172\u001b[0m                  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdepth_paths\u001b[39m\u001b[38;5;124m'\u001b[39m: depth_paths}\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dict_data\n",
      "File \u001b[0;32m~/letsdoit/dataloader/dataloader.py:277\u001b[0m, in \u001b[0;36mDataLoader.get_depths\u001b[0;34m(self, visit_id, asset_type, sample_freq)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_id \u001b[38;5;129;01min\u001b[39;00m video_ids:\n\u001b[1;32m    272\u001b[0m     depths, depths_rotated, depth_paths, intrinsics, poses, orientations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_depths_video_id(visit_id, \n\u001b[1;32m    273\u001b[0m                                                                                                     video_id, \n\u001b[1;32m    274\u001b[0m                                                                                                     asset_type\u001b[38;5;241m=\u001b[39masset_type, \n\u001b[1;32m    275\u001b[0m                                                                                                     sample_freq\u001b[38;5;241m=\u001b[39msample_freq)\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mposes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[1;32m    278\u001b[0m     depths_l \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m depths\n\u001b[1;32m    279\u001b[0m     depths_rotated_l \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m depths_rotated\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "action_objects = pipe.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_objects[0][2].plot_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_objects[0][2].plot_pcd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
